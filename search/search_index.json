{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#a-practical-reference-for-building-glass-to-glass-video-pipelines-on-constrained-hardware","title":"A practical reference for building glass-to-glass video pipelines on constrained hardware","text":"<p>This site exists to explain the code, not to replace it. Right now, the code lives in a single public repository:</p> <ul> <li>https://github.com/astavonin/pi-cam-capture</li> </ul> <p>This repository is the source of truth.</p> <p>The glass-to-glass video pipeline is built incrementally. What exists today is a working foundation: concrete hardware choices, an initial capture path, early transport experiments, and basic observability. Many parts are incomplete and some decisions will change as the system is exercised under real constraints.</p> <p>The goal is to document the system while it is being built, not after it has been polished. Design decisions and failure modes are captured as they happen. If the documentation and the code ever diverge, the code wins.</p>"},{"location":"#how-to-read-this-site","title":"How to read this site","text":"<p>Each article focuses on a narrow, concrete problem and explains the reasoning behind a specific set of changes.</p> <p>Every article links to a GitHub diff that shows the exact code changes discussed. The diff is the authoritative reference. The text exists to explain why those changes were made.</p> <p>Example</p> <p>Like this! Each article starts with such block.</p>"},{"location":"#scope","title":"Scope","text":"<p>This is not a tutorial series or a reference implementation. It assumes familiarity with Linux, V4L2, and video pipelines, and focuses on operational behavior rather than abstractions.</p> <p>The site will change as the code changes. Sections will be revised, approaches will be dropped, and assumptions will be challenged.</p> <p>The intent is to make the system\u2019s evolution visible, not to present a finished design.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/#blog","title":"Blog","text":""},{"location":"part01/","title":"From sensor to memory, correctly and repeatably","text":""},{"location":"part01/#from-sensor-to-memory-correctly-and-repeatably","title":"From sensor to memory, correctly and repeatably","text":""},{"location":"part01/01-rust-on-pi/","title":"Cross-Compiling Rust for Raspberry Pi","text":""},{"location":"part01/01-rust-on-pi/#cross-compiling-rust-for-raspberry-pi","title":"Cross-Compiling Rust for Raspberry Pi","text":"<p>Example</p> <p>If you just want the end result, use the repo diff as a reference: pi-cam-capture?diff.</p> <p>It shows the exact changes covered in this article, as they exist in the repository today, without extra commentary.</p> <p>The goal of this project is straightforward: build a glass-to-glass video pipeline using a Raspberry Pi as a physical device for demo purposes. This article covers the first step, building the foundation that everything else will depend on. At this stage, a USB <code>V4L2</code> camera is enough. It allows exercising the capture path and, more importantly, the build and CI setup, which should be a first step for any long-term project, even a demo one.</p> <p>The capture code here is deliberately boring. It opens <code>/dev/video0</code>, asks for YUYV at 1280\u00d7720, sets up MMAP buffers, and loops over frames. None of that is the hard part. Things get interesting only when the <code>v4l</code> crate pulls in <code>bindgen</code>, and the build has to work for aarch64 as reliably as it does on x86. From that point on, the problem stops being about video and turns into a question of tooling, CI behavior, and how reproducible the environment really is.</p> <p>When evaluating language options for this project, Go naturally comes up early. Its cross-compilation workflow is hard to ignore: producing an ARM binary is usually a one-line operation. That makes it an attractive choice for small utilities and early prototypes, especially when the goal is to move quickly.</p> <pre><code>GOOS=linux GOARCH=arm64 go build\n</code></pre> <p>That simplicity has its price. Garbage collection introduces uncertainty that is difficult to justify in a timing-sensitive video capture loop. C++ avoids that problem, but cross-compiling it for embedded targets often shifts the focus away from the code and onto toolchains, sysroots, and environment quirks. Rust ends up as a more practical compromise. Its build system is less streamlined than Go\u2019s, but it avoids GC pauses and sidesteps much of the operational complexity typical of C++ cross-toolchains. For a Raspberry Pi video-capture codebase, that balance is usually acceptable.</p> <p>So, the plan is to develop on fast <code>x86</code> hardware, reliably ship aarch64 binaries, and avoid relying on the Pi itself for builds.</p>"},{"location":"part01/01-rust-on-pi/#test-app-v4l-camera-capture","title":"Test App: V4L Camera Capture","text":"<p>To start, it makes sense to use something as small as possible while still reflecting real conditions. A minimal Rust application that reads frames from a USB V4L2 camera is enough for that. The intention at this stage is not to build a full pipeline, but to establish a stable baseline for CI and cross-compilation. A simple capture loop is sufficient to surface most toolchain issues early, long before higher-level pipeline logic enters the picture.</p> <p>This keeps the focus where it belongs in the initial phase: not on feature completeness, but on whether the build environment, dependencies, and target architecture behave consistently under cross-compilation.</p> <p></p> <pre><code>let dev = v4l::Device::new(0)?;\n\nlet mut fmt = dev.format()?;\nfmt.width = 1280;\nfmt.height = 720;\nfmt.fourcc = v4l::FourCC::new(b\"YUYV\");\n\nlet _ = dev.set_format(&amp;fmt)?;\nlet mut stream =\n\u00a0 \u00a0\u00a0v4l::io::mmap::Stream::with_buffers(&amp;dev, v4l::buffer::Type::VideoCapture, 4)?;\nwhile let Ok((buf, meta)) = stream.next() {\n\u00a0 \u00a0\u00a0println!(\"size={} seq={} ts={}\", buf.len(), meta.sequence, meta.timestamp);\n}\n</code></pre> <p>As expected, even this minimal example does not cross-compile cleanly out of the box. The combination of the v4l crate, its reliance on bindgen, and the requirement for a compatible libclang version is enough to expose problems in the build environment. This is precisely the kind of friction that needs to be addressed early, before the codebase grows beyond a simple capture loop.</p>"},{"location":"part01/01-rust-on-pi/#why-not-just-gcc-aarch64-linux-gnu","title":"Why Not Just\u00a0gcc-aarch64-linux-gnu?","text":"<p>You can\u00a0easily\u00a0wire the toolchain manually. It usually starts like this:</p> <pre><code>sudo apt install gcc-aarch64-linux-gnu\nrustup target add aarch64-unknown-linux-gnu\n</code></pre> <p>Then you point Cargo at the cross-linker:</p> <pre><code># .cargo/config.toml\n\n[target.aarch64-unknown-linux-gnu]\nlinker = \"aarch64-linux-gnu-gcc\"\n</code></pre> <p>At this\u00a0point,\u00a0you can:</p> <pre><code>cargo build --target aarch64-unknown-linux-gnu\n</code></pre> <p>This setup may work at a given point in time, but it is fragile. The linker alone is not enough; <code>bindgen</code> also depends on a compatible <code>libclang</code>, along with the correct headers and sysroot. Any mismatch between these components is enough to break the build, usually resulting in a cascade of <code>libclang</code> errors. This kind of brittleness is the main reason to move toward a dedicated, containerized cross-compilation image instead of relying on ad-hoc host configuration.</p> <p>By contrast, moving the entire toolchain into a dedicated cross-compilation image turns it into a versioned artifact. The linker, sysroot, <code>libclang</code>, and all supporting packages are bundled into a single Docker image that can be rebuilt, tagged, and reused. From the project\u2019s perspective, the only requirement is a <code>Cross.toml</code> file pointing at a specific image, such as <code>ghcr.io/astavonin/cross-aarch64:llvm12</code>. Both local builds and CI then run against the exact same environment. This approach avoids the fragility of host-level configuration and is the main reason a containerized setup is preferred over a manually assembled <code>gcc-aarch64-linux-gnu</code> toolchain.</p>"},{"location":"part01/01-rust-on-pi/#building-a-custom-cross-image","title":"Building a Custom\u00a0cross Image","text":"<p>The natural question is, why not just use the stock\u00a0<code>ghcr.io/cross-rs/aarch64-unknown-linux-gnu</code>\u00a0image? It works for many projects, and for simple crates, it\u2019s perfectly fine. The problem shows up the moment the build pulls in\u00a0<code>v4l</code>, because that drags in\u00a0<code>bindgen</code>, and\u00a0<code>bindgen</code>\u00a0depends on\u00a0<code>libclang</code>. The default cross image ships an older <code>libclang</code> that doesn\u2019t play well with some of the functions\u00a0<code>bindgen</code>\u00a0expects. That\u2019s how you end up with the familiar error:</p> <pre><code>A libclang function was called that is not supported\ncalled function = clang_getTranslationUnitTargetInfo\n</code></pre> <p>I started from the official cross-rs base:</p> <pre><code>FROM ghcr.io/cross-rs/aarch64-unknown-linux-gnu:latest\n</code></pre> <p>This image\u00a0is based\u00a0on Ubuntu 16.04 Xenial, which becomes\u00a0a critical part in components selection.\u00a0Xenial is\u00a0old, and the\u00a0available LLVM packages\u00a0for it\u00a0are limited.\u00a0If you look at: https://apt.llvm.org/xenial/dists/, the highest LLVM/Clang version published for Xenial is 12. That\u2019s the ceiling.\u00a0Anything newer doesn\u2019t exist for this base distribution; we can use PPA if needed, but 12 is enough for my case.</p> <p>So the choice of Clang wasn\u2019t aesthetic \u2014 the platform dictated it. LLVM 12 is the newest supported toolchain you can install inside Xenial without rebuilding LLVM from source, and it\u2019s new enough to keep <code>bindgen</code> happy for <code>v4l</code>.</p> <p>The Dockerfile installs the LLVM 12 family and exposes it to bindgen:</p> <pre><code>ENV LIBCLANG_PATH=/usr/lib/llvm-12/lib\nENV CLANG_PATH=/usr/bin/clang-12\nENV LLVM_CONFIG_PATH=/usr/bin/llvm-config-12\n</code></pre>"},{"location":"part01/01-rust-on-pi/#configuring-rust-projects-with-crosstoml","title":"Configuring Rust Projects with\u00a0<code>Cross.toml</code>","text":"<p>Once the custom image is available, integrating it into a Rust project is straightforward. All configuration is contained in a small <code>Cross.toml</code> file at the root of the repository. This file specifies which Docker image should be used for <code>aarch64</code> builds, ensuring that both local builds and CI run against the same toolchain and environment.</p> <p>A minimal configuration looks like this:</p> <pre><code>[target.aarch64-unknown-linux-gnu]\nimage = \"ghcr.io/astavonin/cross-aarch64:llvm12\"\n\n[build.env]\npassthrough = [\"RUST_BACKTRACE\"]\n</code></pre> <p>That\u2019s all you need. Now the actual build becomes:</p> <pre><code>cross build --release --target aarch64-unknown-linux-gnu\n</code></pre> <p>And under the hood, cross will:</p> <ol> <li>Pull or reuse your custom image with the correct <code>libclang</code>.</li> <li>Mount the Rust project inside the container.</li> <li>Invoke <code>cargo build</code> using the aarch64 toolchain baked into the image.</li> </ol> <p>The important part here is that the toolchain is no longer spread across multiple machines or CI runners. Everything that matters \u2014 linker, sysroot, libclang, headers \u2014 travels together inside a versioned container.</p>"},{"location":"part01/01-rust-on-pi/#ci-strategy-fast-x86-builds-on-demand-arm-artifacts","title":"CI Strategy: Fast x86 Builds + On-Demand ARM Artifacts","text":"<p>CI resources are limited, so build time needs to be used carefully. Running native x86 builds on GitHub Actions provides fast feedback from unit and integration tests without paying the overhead of container startup or invoking the aarch64 toolchain. For feature branches and routine development, this level of validation is sufficient and keeps the feedback loop short.</p> <p></p> <p>Cross-compiling for ARM adds real overhead: container startup, toolchain initialization, and <code>bindgen</code> running against the correct <code>libclang</code>. This is reasonable when an ARM artifact is actually needed, but it is unnecessary work for every feature branch. A common compromise is to run fast native x86 builds on branches, and run aarch64 cross-builds only on <code>main</code>, where release artifacts are produced. This keeps CI feedback fast while still guaranteeing that the deployable binary is built in a reproducible ARM environment.</p> <p>This split keeps the feedback loop short while still producing reliable ARM binaries when they matter. It also avoids any dependency on native ARM runners, which are typically slower or require additional infrastructure. All builds run on standard x86 GitHub runners, with the custom cross image ensuring that ARM artifacts are produced in a consistent and reproducible environment.</p> <p>With the toolchain in place and the CI behaving predictably, it's time to move on to the parts of the project that actually matter. The tiny capture app still lacks many functionalities. Next comes testing with the native Pi camera modules, measuring queue depth, watching how the system behaves under thermal load, and shaping the first real version of the video pipeline.</p>"},{"location":"part01/02-emulation-and-video-testing/","title":"Integration Testing for Linux Video Pipelines","text":""},{"location":"part01/02-emulation-and-video-testing/#integration-testing-for-linux-video-pipelines","title":"Integration Testing for Linux Video Pipelines","text":"<p>Example</p> <p>If you just want the end result, use the repo diff as a reference: pi-cam-capture?diff.</p> <p>It shows the exact changes covered in this article, as they exist in the repository today, without extra commentary.</p> <p>Most embedded Linux projects end up with the same testing setup. Unit tests cover parts of the logic, often in a patchy way, because syscalls and OS behavior are out of scope. Hardware-in-the-loop is used to check that the system works on the real device. Even in theory, this combination is weak and fragile, but in many teams it is still treated as \"good enough\". The blind spot is always the same. The boundary between user space and the kernel is rarely exercised during normal development, even though that is where a large share of real failures occur.</p> <p>In Linux-based video systems, most application code does not manipulate pixels directly. It involves negotiating formats via ioctls, allocating and mapping buffers, handling blocking semantics, and interpreting metadata produced by drivers. This boundary is where failures cluster, and it is precisely the part of the system that neither unit tests nor HIL handles well.</p> <p>Unit tests cannot exercise kernel behavior in any meaningful way. In the best case, they replace file descriptors, ioctls, and device state with mocks, which is fine for checking control flow and error handling. What disappears is the behavior that actually matters in video processing. Buffer queues have ordering and lifetime rules. Some ioctls return success while silently adjusting parameters. Others block depending on the driver's state. Timestamps have specific origins and invariants. These details define how a video device behaves, and a mock cannot realistically reproduce them.</p> <p>And quite often the result is predictable. Code passes unit tests while embedding incorrect assumptions about alignment, stride, buffer lifetime, or sequencing. These assumptions remain invisible until the first interaction with a real device.</p> <p>At that point, teams fall back to hardware-in-the-loop testing. HIL does exercise the real kernel interface, but it usually happens late and at a high cost. Access to hardware is often limited, setup can be slow, failures may be non-deterministic, and reproducing issues relies heavily on the physical context. When integration problems arise only at this stage, the iteration process can slow down significantly. Continuous Integration (CI) provides little support in this scenario. While it is sometimes possible to wire HIL into a CI pipeline, it tends to be fragile and hard to maintain reliability, which means these paths are not exercised consistently, and regressions reappear.</p> <p>This creates a systemic blind spot. Unit tests validate logic in an environment that cannot fail in realistic ways. HIL validates behavior in an environment that is expensive and difficult to automate. The kernel boundary, where most integration bugs originate, remains largely untested during normal development. For Linux video systems, this is not an edge case. It is the dominant failure mode.</p> <p>This gap is structural, not accidental. It comes from how different test layers interact with the system stack and which boundaries they actually cross.</p> <p></p> <p>The diagram shows how coverage is distributed across a Linux video, or any other typical embedded system. Unit tests remain confined to application logic and never cross into the kernel. Hardware-in-the-loop testing exercises the full stack, but it sits outside the normal development loop and is expensive to run continuously. Virtual-device integration tests occupy the middle ground. They drive real syscalls and kernel code paths through emulated drivers, allowing the user-space-to-kernel boundary to be exercised early, repeatedly, and in CI, without relying on physical hardware.</p>"},{"location":"part01/02-emulation-and-video-testing/#why-linux-is-unusually-amenable-to-this-kind-of-integration-testing","title":"Why Linux is unusually amenable to this kind of integration testing","text":"<p>Integration testing with virtual devices usually works on Linux because most hardware interaction already goes through the kernel. From the user space, the boundary is not registers or buses. It is file descriptors, ioctls, memory mappings, and kernel metadata. These boundaries are explicit, documented, and often straightforward to virtualize.</p> <p>For video capture, the contract is V4L2. V4L2 allows us to discover devices via <code>/dev/video*</code> and to obtain capabilities, formats, and controls via standardized ioctls. Buffers are exchanged via MMAP or USERPTR with well-defined lifetime rules. Metadata such as timestamps and sequence numbers follow documented semantics. At the end, from the user space, a physical camera and a virtual camera look identical as long as they speak the same kernel API. In practice, this is visible directly through standard V4L2 tooling. A real camera and a virtual device appear identical at the kernel boundary.</p> <pre><code>\u279c v4l2-ctl --list-devices\nvivid (platform:vivid-000):\n        /dev/video2\n\nIntegrated Camera: Integrated C (usb-0000:00:14.0-6):\n        /dev/video0\n</code></pre> <p>Such contracts are a key difference from bare-metal or tightly coupled firmware systems. In Linux, the kernel already sits between your code and the hardware, enforcing an abstraction. That abstraction can often be backed by a real driver or a virtual one with identical user-space behavior. As long as the driver implements the same V4L2 contract, the application code exercises the same paths.</p> <p>Linux also exposes enough system state to enable deterministic discovery and selection. Sysfs provides device identity, driver names, and topology under <code>/sys/class</code>. Procfs exposes the runtime context that affects behavior. Instead of guessing which <code>/dev/video0</code> is present, code and tests can identify devices by driver and capabilities, and fail explicitly when expectations are not met.</p> <p>This makes Linux an incredibly practical test harness that helps to keep your codebase stable. Your integration tests are production-like in the sense that you can run real syscalls, negotiate real formats, allocate real buffers, and observe real blocking behavior without needing physical hardware attached. The kernel enforces the same rules either way.</p> <p>For video systems, this matters in practice. Most integration failures are not sensor-specific. They come from incorrect assumptions about format negotiation, buffer handling, timing, or sequencing. These failures live at the kernel interface. Exercising that interface deterministically catches a large class of bugs early.</p>"},{"location":"part01/02-emulation-and-video-testing/#making-the-environment-deterministic","title":"Making the environment deterministic","text":"<p>Integration tests that touch kernel interfaces only make sense if the environment is predictable. If setup depends on manual steps or undocumented assumptions, failures become ambiguous and easy to dismiss. This is where many teams stop trusting integration tests altogether.</p> <p>For V4L2, the environment includes user space tools, kernel support, and the way devices appear under <code>/dev</code> and <code>/sys</code>. In this project, those assumptions are encoded directly in a setup script rather than documented separately.</p> <p>The script starts by installing and verifying the required user space tooling. If inspection tools are missing, there is no point in proceeding.</p> <pre><code>for cmd in v4l2-ctl ffmpeg; do\n    if command -v \"$cmd\" &amp;&gt;/dev/null; then\n        success \"$cmd installed\"\n    else\n        error \"$cmd not found\"\n        ok=false\n    fi\ndone\n</code></pre> <p>Kernel support is checked explicitly. Virtual devices only work if the driver is available for the running kernel, and this varies by distribution.</p> <pre><code>for mod in vivid v4l2loopback; do\n    if modinfo \"$mod\" &amp;&gt;/dev/null; then\n        success \"$mod module available\"\n    else\n        warn \"$mod module not available\"\n    fi\ndone\n</code></pre> <p>Loading the virtual camera is a deliberate step with fixed parameters. The configuration is part of the test contract, not an implementation detail.</p> <pre><code>$SUDO modprobe vivid n_devs=2 node_types=0x1,0x1 input_types=0x81,0x81\n</code></pre> <p>After loading the module, the script validates the result from user space. Tests do not assume devices exist; they require them.</p> <pre><code>$SUDO v4l2-ctl --list-devices\n</code></pre> <p>At this point, the environment is in a known state. Virtual video devices exist, they are discoverable through standard V4L2 tooling, and they expose the same kernel interfaces as physical cameras. Integration tests can rely on this without special casing or fallback logic.</p> <p>The script also configures deterministic test patterns on each vivid device, using sysfs to identify them and <code>v4l2-ctl</code> to apply known formats and patterns.</p> <pre><code>if v4l2-ctl -d \"$dev\" --set-fmt-video=width=640,height=480,pixelformat=YUYV; then\n    success \"$dev: Set format to 640x480 YUYV\"\nfi\n\nv4l2-ctl -d \"$dev\" --set-ctrl=test_pattern=\"$pattern\"\n</code></pre> <p>This ensures that integration tests see predictable frame content, not just \u201csome bytes\u201d.</p> <p>Finally, teardown is explicit. Kernel state is global, and CI runs must clean up after themselves.</p> <p>Warning</p> <p>Kernel state is global. It is safe to run multiple integration tests in parallel on a runner that has been preconfigured with <code>vivid</code> already loaded. CI jobs that load or unload kernel modules must not run concurrently on the same runner.</p> <pre><code>$SUDO modprobe -r vivid\n</code></pre> <p>The result is a setup that either succeeds and is validated, or fails immediately. When an integration test fails, the failure comes from the code under test, not from an unclear or partially configured environment.</p>"},{"location":"part01/02-emulation-and-video-testing/#using-kernel-provided-virtual-devices-vivid","title":"Using kernel-provided virtual devices (<code>vivid</code>)","text":"<p>With a deterministic environment in place, the next question is what the integration tests should talk to. For V4L2, the choice is straightforward. Mock devices in user space do not exercise the kernel boundary. Physical cameras reintroduce cost and instability. The practical middle ground is to use a kernel driver that implements the same V4L2 contract as real hardware.</p> <p>On Linux, that driver is <code>vivid</code>.</p> <p>From user space, the application does not talk to hardware directly. It talks to the V4L2 API. Whether the request is handled by a physical camera driver or a virtual one is an implementation detail hidden behind the kernel interface.</p> <p></p> <p>This is why vivid works for integration testing. It registers <code>/dev/video*</code> nodes and implements the same ioctls, buffer APIs, and metadata semantics as real capture devices. The code under test follows the same execution paths either way.</p> <p>This equivalence is visible directly through standard tooling. A physical camera and a vivid device are discovered in the same way:</p> <pre><code>\u279c v4l2-ctl --list-devices\nvivid (platform:vivid-000):\n        /dev/video2\n\nIntegrated Camera: Integrated C (usb-0000:00:14.0-6):\n        /dev/video0\n</code></pre> <p>Capabilities are queried through the same ioctls and expose the same capture and streaming flags:</p> <pre><code>\u279c v4l2-ctl --device=/dev/video0 --all\nDriver name   : uvcvideo\nCapabilities  : Video Capture, Streaming\n</code></pre> <pre><code>\u279c v4l2-ctl --device=/dev/video2 --all\nDriver name   : vivid\nCapabilities  : Video Capture, Streaming\n</code></pre> <p>Format negotiation follows the same path as well:</p> <pre><code>\u279c v4l2-ctl -d /dev/video2 --list-formats\n[0]: 'YUYV' (YUYV 4:2:2)\n</code></pre> <p>From the application\u2019s point of view, there is no special-case logic here. Devices are opened the same way, formats are negotiated through the same ioctls, and frames are streamed using the same buffer APIs. If the driver speaks V4L2 correctly, user space does not care whether frames originate from a sensor or from a kernel generator.</p> <p>This makes vivid useful for integration testing, but also defines its limits. It does not model sensor quirks, ISP behavior, or vendor-specific timing details. Those belong in hardware-in-the-loop testing. What vivid does cover is the V4L2 surface itself: format negotiation, buffer handling, sequencing, and timing semantics. That is where many integration failures originate.</p>"},{"location":"part01/02-emulation-and-video-testing/#structuring-integration-tests-around-virtual-devices","title":"Structuring integration tests around virtual devices","text":"<p>Once vivid is available, the main job is keeping integration tests honest. They should fail when the runner is not prepared, and they should not depend on accidental details like <code>/dev/video0</code> ordering.</p> <p>In the repo, that logic lives in one place: a sysfs-backed discovery function plus a couple of \u201chard requirement\u201d macros. The discovery code reads <code>/sys/class/video4linux/video*/name</code>, filters for devices that contain <code>vivid</code>, then verifies the device can actually be opened before returning it. That avoids probing real cameras and makes failures deterministic.</p> <pre><code>// This is a snippet but NOT a real code.\nfn vivid_devices() -&gt; Vec&lt;u32&gt; {\n    (0..10)\n        .filter(|i| {\n            let name = std::fs::read_to_string(\n                format!(\"/sys/class/video4linux/video{}/name\", i)\n            ).unwrap_or_default();\n\n            name.to_lowercase().contains(\"vivid\")\n                &amp;&amp; V4L2Device::open(*i).is_ok()\n        })\n        .collect()\n}\n</code></pre> <p>The tests deliberately do not skip when vivid is missing. They panic with an actionable error, because this is the class of failures CI must catch early. The single-device macro is used for general \u201ccan open / can stream\u201d tests.</p> <pre><code>// This is a snippet but NOT a real code.\nmacro_rules! require_vivid {\n    () =&gt; {\n        vivid_devices()\n            .first()\n            .copied()\n            .expect(\"vivid device not found; run dev-setup.sh load-vivid\")\n    };\n}\n</code></pre> <p>Some tests depend on two devices, because <code>dev-setup.sh</code> configures them with different patterns. That is where <code>require_vivid_pair!()</code> comes in: it asserts a pair exists and returns <code>(devices[0], devices[1])</code>. If the runner only has one vivid node, or vivid was loaded with the wrong parameters, it fails loudly.</p> <pre><code>// This is a snippet but NOT a real code.\nmacro_rules! require_vivid_pair {\n    () =&gt; {{\n        let devs = vivid_devices();\n        assert!(\n            devs.len() &gt;= 2,\n            \"expected two vivid devices; check vivid module configuration\"\n        );\n        (devs[0], devs[1])\n    }};\n}\n</code></pre> <p>From there, the test body stays intentionally boring: open device, set a known format, create a small stream, pull frames, validate invariants. The format choice is fixed (<code>640x480 YUYV</code>) so that frame sizing and pixel access are stable across runs.</p> <pre><code>let mut device = V4L2Device::open(device_index).expect(\"Failed to open vivid device\");\n\nlet requested = Format::new(640, 480, FourCC::YUYV);\nlet actual = device.set_format(&amp;requested).expect(\"Failed to set format\");\n\nlet mut stream = device.create_stream(4).expect(\"Failed to create stream\");\nlet frame = stream.next_frame().expect(\"Failed to capture frame\");\n</code></pre> <p>One operational detail matters here: these tests are marked <code>#[serial]</code>. Kernel-backed devices are shared state. Even if the runner is \u201cpreconfigured\u201d, concurrent open/stream/close sequences against the same nodes tend to create noise. Serializing keeps failures attributable.</p> <pre><code>#[test]\n#[serial]\nfn test_vivid_capture_single_frame() { /* ... */ }\n</code></pre> <p>The pattern-specific tests show why having a pair matters. The first device is expected to be configured as a gradient, the second as color bars, and the validations fail with messages that point back to the setup script.</p> <pre><code>let (gradient_device, _) = require_vivid_pair!();\n// ...\nlet result = validate_gradient(&amp;frame, &amp;format);\nassert!(result.is_ok(), \"Gradient validation failed on first vivid device...\");\n</code></pre> <p>This structure keeps the integration suite predictable: the environment requirements are explicit, discovery is deterministic, and the tests exercise the same public API surface as production code.</p>"},{"location":"part01/02-emulation-and-video-testing/#validating-frame-content-and-metadata","title":"Validating frame content and metadata","text":"<p>Streaming frames is a weak signal. A lot of broken pipelines still stream.</p> <p>The typical failure is boring and expensive: format negotiation succeeds, buffers flow, sizes look right, and nothing errors out. Then someone notices the image is wrong, or timestamps jump, or sequence numbers reset under load. By that point it is usually on hardware, and it is already hard to localize.</p> <p>Virtual devices help because the output is predictable. That makes it reasonable to assert properties of frames instead of just checking that data exists.</p> <p>Start with metadata. Sequence numbers should increase. Timestamps should be present and ordered. If this fails, the pipeline is already violating assumptions downstream components rely on.</p> <pre><code>let frames: Vec&lt;Frame&gt; = (0..5)\n    .map(|_| stream.next_frame().expect(\"capture failed\"))\n    .collect();\n\nfor w in frames.windows(2) {\n    assert!(w[1].sequence &gt; w[0].sequence);\n    assert!(w[1].timestamp &gt;= w[0].timestamp);\n}\n</code></pre> <p>Content checks can stay minimal. vivid can generate fixed patterns. A couple of pixel samples are enough to catch the usual class of mistakes: wrong stride, wrong pixel format assumptions, or broken conversion logic.</p> <pre><code>let (r1, _, _) = frame.pixel_at(10, 10).unwrap();\nlet (r2, _, _) = frame.pixel_at(200, 10).unwrap();\nassert!(r2 &gt; r1);\n</code></pre> <p>This is not image quality testing. It is a sanity check that the pipeline interprets the kernel\u2019s buffers the way it thinks it does. Once metadata and minimal content checks are in place, the integration suite stops answering \u201cdid we get a frame\u201d and starts answering \u201cdid we handle the frame correctly\u201d.</p> <p>Virtual-device integration tests do not replace unit tests or hardware-in-the-loop testing. Unit tests still cover logic and edge cases. HIL still covers hardware-specific behavior. The integration layer sits in between, exercising real syscalls, ioctls, buffer queues, and timing semantics without depending on physical devices. This catches kernel-boundary failures early and pushes HIL back to what it is good at, rather than using it as the first line of integration testing.</p>"}]}